{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixel-wise classification\n",
    "\n",
    "# Img Restoration : + > Concat     흐릿한 사진에서 노이즈 부분을 빼주면 선명해짐\n",
    "# Img Classification : + < Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T04:54:47.703784Z",
     "start_time": "2021-06-18T04:54:47.160976Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import os.path as osp\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ./resources/Kitti/data_road/training/image_2/um_000081.png <----- source\n",
    "- ./resources/Kitti/data_road/training/gt_image_2/um_road_000081.png <----- target\n",
    "\n",
    "![source](./resources/Kitti/data_road/training/image_2/um_000081.png \"Variable\")\n",
    "![target](./resources/Kitti/data_road/training/gt_image_2/um_road_000081.png \"Variable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T23:48:52.358367Z",
     "start_time": "2021-06-16T23:48:52.347284Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training/image_2/um_000000.png training/gt_image_2/um_road_000000.png\n",
      "\n",
      "training/image_2/um_000000.png training/gt_image_2/um_road_000000.png\n",
      "['training/image_2/um_000000.png', 'training/gt_image_2/um_road_000000.png']\n",
      "training/image_2/um_000001.png training/gt_image_2/um_road_000001.png\n",
      "\n",
      "training/image_2/um_000001.png training/gt_image_2/um_road_000001.png\n",
      "['training/image_2/um_000001.png', 'training/gt_image_2/um_road_000001.png']\n",
      "training/image_2/um_000002.png training/gt_image_2/um_road_000002.png\n",
      "\n",
      "training/image_2/um_000002.png training/gt_image_2/um_road_000002.png\n",
      "['training/image_2/um_000002.png', 'training/gt_image_2/um_road_000002.png']\n"
     ]
    }
   ],
   "source": [
    "imgsets_file = osp.join('resources/Kitti', '%s.txt'% 'train')\n",
    "\n",
    "for i, src_tar_name in enumerate(open(imgsets_file)):\n",
    "    if i < 3:\n",
    "        print(src_tar_name)\n",
    "        src_tar_name = src_tar_name.strip() # 양옆의 공백을 제거\n",
    "        print(src_tar_name)\n",
    "        src_tar_name = src_tar_name.split() # 띄워쓰기를 나눠서 List로 만듬\n",
    "        print(src_tar_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T04:55:07.263781Z",
     "start_time": "2021-06-18T04:55:07.241196Z"
    }
   },
   "outputs": [],
   "source": [
    "class KITTIdataset(torch.utils.data.Dataset):\n",
    "    class_names = np.array(['background', 'road'])\n",
    "    mean_bgr = np.array([104.00698793, 116.66876762, 122.67891434]) # KITTIdataset의 Normalization\n",
    "\n",
    "    def __init__(self, root, split='train', transform=False): # root폴더 => resources\n",
    "        self._transform = transform\n",
    "        dataset_dir = os.path.join(root, 'Kitti') #string을 /로 이어서 경로 만들어주는 것\n",
    "        self.files = []  # dict 형태로 pair로 append({\"img\" : src_tar_name[0], \"lbl\" : src_tar_name[1]})\n",
    "        # TODO\n",
    "        # file names in 'self.files'\n",
    "        imgsets_file = os.path.join(dataset_dir, '%s.txt'% split)\n",
    "        \n",
    "        for i, src_tar_name in enumerate(open(imgsets_file)):\n",
    "            src_tar_name = src_tar_name.strip()\n",
    "            src_tar_name = src_tar_name.split()\n",
    "            self.files.append({\"img\" : os.path.join(dataset_dir, \"data_road\",  src_tar_name[0]), \"lbl\" : os.path.join(dataset_dir, \"data_road\", src_tar_name[1]) })\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index): # Loader로 부터 들어온 데이터의 해당하는 이름을 찾아서 읽어서 내보내주는 역할\n",
    "        # load image & label\n",
    "        # TODO\n",
    "        # image -> img, label -> lbl\n",
    "        data_file = self.files[index]\n",
    "        img_file = data_file[\"img\"]\n",
    "        lbl_file = data_file[\"lbl\"]\n",
    "        \n",
    "        img = PIL.Image.open(img_file)       # PIL : Python Image Library\n",
    "        img = np.array(img, dtype=np.uint8)\n",
    "        lbl = PIL.Image.open(lbl_file)\n",
    "        lbl = np.array(lbl, dtype=np.int32)\n",
    "        lbl[lbl == 255] = 1 # background는 0이고 도로는 1로 주기 위해서\n",
    "        \n",
    "        img, lbl = self.transform(img, lbl)\n",
    "        return img, lbl\n",
    "\n",
    "        # numpy와 tensor의 배열 차원 순서가 다르다. \n",
    "        # numpy : (행, 열, 채널)\n",
    "        # tensor : (채널, 행, 열)\n",
    "        # 따라서 위 순서에 맞춰 transpose\n",
    "    \n",
    "    def transform(self, img, lbl):\n",
    "        img = img[:, :, ::-1]  # RGB -> BGR\n",
    "        img = img.astype(np.float64)\n",
    "        img -= self.mean_bgr\n",
    "        img = img.transpose(2, 0, 1) # H W C -> C H W\n",
    "        img = torch.from_numpy(img).float()\n",
    "        lbl = torch.from_numpy(lbl).long()\n",
    "        return img, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T04:55:35.136957Z",
     "start_time": "2021-06-18T04:55:08.142266Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/piai/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a423baa27c2a4b7da5f431fd3851c777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(KITTIdataset(root = 'resources', split = 'train', transform = True), \n",
    "                                           batch_size = 1, shuffle = True) # 이미지 크기가 제각각이라서 batch_size는 1\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(KITTIdataset(root = 'resources', split = 'val', transform = True), \n",
    "                                         batch_size = 1, shuffle = False)\n",
    "\n",
    "vgg16 = torchvision.models.vgg16(pretrained = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define the Network\n",
    "-VGG16\n",
    "\n",
    "- FCN model\n",
    "![convnet](./resources/fcn_upsampling.png \"Variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-16T23:48:55.126649Z",
     "start_time": "2021-06-16T23:48:55.102652Z"
    },
    "code_folding": [
     1
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FCN에서는 Input 이미지의 사이즈를 고려하지 않는다. Convolution의 특징. 이미지 사이즈가 달라도 상관없다.\n",
    "# Unet도 FCN이라서 동일함, in_channel 수, out_channel 수, kernel_size만 고려\n",
    "# FCN : Sum, Unet : Concat, Convolution을 하면서 Kernel이 2개를 어떻게 합쳤는지 학습하게된다.\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, num_class = 21):\n",
    "        super(FCN, self).__init__()\n",
    "        \n",
    "        ## Why padding 100?? https://github.com/shelhamer/fcn.berkeleyvision.org\n",
    "        # The 100 pixel input padding guarantees that the network output can be aligned to the input for any input size in the given datasets\n",
    "        self.features1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding = 100),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(64, 64, 3, padding = 1),\n",
    "            nn.ReLU(inplace = True))\n",
    "    \n",
    "        self.features2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(128, 128, 3, padding = 1),\n",
    "            nn.ReLU(inplace = True))\n",
    "        \n",
    "        self.features3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(256, 256, 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(256, 256, 3, padding = 1))\n",
    "        \n",
    "        self.features4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(512, 512, 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(512, 512, 3, padding = 1))\n",
    "                \n",
    "        self.features5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(512, 512, 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(512, 512, 3, padding = 1))\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(2, stride = 2, ceil_mode = True)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(512, 4096, 7),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout2d(),\n",
    "            nn.Conv2d(4096, 4096, 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout2d(),\n",
    "            nn.Conv2d(4096, num_class, 1))\n",
    "        \n",
    "        self.upscore2 = nn.ConvTranspose2d(num_class, num_class, kernel_size = 4, stride = 2, bias = False)\n",
    "        self.upscore4 = nn.ConvTranspose2d(num_class, num_class, kernel_size = 4, stride = 2, bias = False)\n",
    "        self.upscore8 = nn.ConvTranspose2d(num_class, num_class, kernel_size = 16, stride = 8, bias = False)\n",
    "        \n",
    "        self.score_pool4 = nn.Conv2d(512, num_class, 1)\n",
    "        self.score_pool3 = nn.Conv2d(256, num_class, 1)\n",
    "        \n",
    "        self.params = [self.features1, self.features2, self.features3, \n",
    "                       self.features4, self.features5]\n",
    "        \n",
    "    def upsample(self, x, size):\n",
    "        return nn.functional.upsample(x, size = size, mode = 'bilinear')\n",
    "                             \n",
    "    def forward(self, inputs):\n",
    "        x = self.features1(inputs)\n",
    "        pool1 = self.maxpool(x)\n",
    "        x = self.features2(pool1)\n",
    "        pool2 = self.maxpool(x)\n",
    "        x = self.features3(pool2)\n",
    "        pool3 = self.maxpool(x)\n",
    "        x = self.features4(pool3)\n",
    "        pool4 = self.maxpool(x)\n",
    "        x = self.features5(pool4)\n",
    "        pool5 = self.maxpool(x)\n",
    "        x = self.classifier(pool5)\n",
    "        \n",
    "        # also use getattr with for loop ...\n",
    "        x = self.upscore2(x)\n",
    "        \n",
    "        pool4 = self.score_pool4(pool4)\n",
    "        pool4 = pool4[:, :, 5:5 + x.size()[2], 5:5 + x.size()[3]]\n",
    "        x = torch.add(x, pool4)\n",
    "        \n",
    "        x = self.upscore4(x)\n",
    "        \n",
    "        pool3 = self.score_pool3(pool3)\n",
    "        pool3 = pool3[:, :, 9:9 + x.size()[2], 9:9 + x.size()[3]]\n",
    "        x = torch.add(x, pool3)\n",
    "        \n",
    "        x = self.upscore8(x)\n",
    "        x = x[:, :, 33:33 + inputs.size()[2], 33:33 + inputs.size()[3]]\n",
    "        return x\n",
    "    \n",
    "    def copy_params(self, vgg):  # 학습된 vgg weight를 가져와서 FCN과 같은 Structure를 짜놓고 Parameter weight를 덮어씌우는 방법\n",
    "        for l1, l2 in zip(vgg.features, self.params):\n",
    "            if (isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d)):\n",
    "                assert l1.weight.size() == l2.weight.size()\n",
    "                assert l1.bias.size() == l2.bias.size()\n",
    "                l2.weight.data = l1.weight.data\n",
    "                l2.bias.data = l1.bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# U-Net\n",
    "\n",
    "\n",
    "-  U-Net model\n",
    "![unet](./resources/unet.png \"Variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "EncoderBlock layers\n",
    "    \n",
    "    - conv2d in -> out (kernel = 3\n",
    "    - batchnorm2d\n",
    "    - relu\n",
    "    - conv2d out -> out (kernel = 3\n",
    "    - batchnorm2d\n",
    "    - relu\n",
    "    - dropout\n",
    "    - maxpool (kernel = 2, sride = 2)\n",
    "\n",
    "DecoderBlock layers\n",
    "    \n",
    "    - conv2d in -> middle (kernel = 3)\n",
    "    - batchnorm2d\n",
    "    - relu\n",
    "    - conv2d middle -> middle (kernel = 3)\n",
    "    - batchnorm2d\n",
    "    - relu\n",
    "    - convtranspose2d middle -> out (kernel = 3, stride = 2) #이미지 사이즈 2배가 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T04:55:46.924453Z",
     "start_time": "2021-06-18T04:55:46.896696Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class _EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout=False):\n",
    "        super(_EncoderBlock, self).__init__()\n",
    "        # TODO\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True), # inplace=True 기존거에서 작업하니깐 2중으로 메모리가 들어가지않는다.\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout())\n",
    "        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.encode = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encode(x)\n",
    "\n",
    "# Convolution은 pixel을 모은다면 DeConvolution은 1pixel을 다시 펼치는 작업\n",
    "class _DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super(_DecoderBlock, self).__init__()\n",
    "        # TODO\n",
    "        self.decode = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, middle_channels, kernel_size = 3),\n",
    "            nn.BatchNorm2d(middle_channels),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(middle_channels, middle_channels, kernel_size = 3),\n",
    "            nn.BatchNorm2d(middle_channels),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size = 2, stride = 2) # Deconvolution을 거치게되면 Img사이즈가 커짐.\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decode(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module): # 네트워크가 U자로 생겼다\n",
    "    def __init__(self, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.enc1 = _EncoderBlock(3, 64)\n",
    "        self.enc2 = _EncoderBlock(64, 128)\n",
    "        self.enc3 = _EncoderBlock(128, 256)\n",
    "        self.enc4 = _EncoderBlock(256, 512, dropout=True)\n",
    "        self.center = _DecoderBlock(512, 1024, 512) \n",
    "        self.dec4 = _DecoderBlock(1024, 512, 256) # input채널 1024 => skip으로 넘어오는 채널 512 +  up-conv로 진행된 채널 512\n",
    "        self.dec3 = _DecoderBlock(512, 256, 128)\n",
    "        self.dec2 = _DecoderBlock(256, 128, 64)\n",
    "        self.dec1 = nn.Sequential( # Convolution\n",
    "            nn.Conv2d(128, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.final = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        enc4 = self.enc4(enc3)\n",
    "        center = self.center(enc4)\n",
    "        dec4 = self.dec4(torch.cat([center, F.upsample(enc4, center.size()[2:], mode='bilinear')], 1)) # encoding feature를 Concat,     center.size => [1, 512, 30, 138], 3번째부터니깐 [30, 138]\n",
    "        print(F.upsample(enc4, center.size()[2:]).size())\n",
    "        dec3 = self.dec3(torch.cat([dec4, F.upsample(enc3, dec4.size()[2:], mode='bilinear')], 1))     # dec4.size() => [1, 256, 52, 268] 3번째부터니깐 [52, 268]\n",
    "        dec2 = self.dec2(torch.cat([dec3, F.upsample(enc2, dec3.size()[2:], mode='bilinear')], 1))     # input에서 high-level feature로 가기 전에 low-level featrure를 함께써서 resolution이 올라가면서 특성을 살릴 수 있다.\n",
    "        dec1 = self.dec1(torch.cat([dec2, F.upsample(enc1, dec2.size()[2:], mode='bilinear')], 1))\n",
    "        final = self.final(dec1)\n",
    "        return F.upsample(final, x.size()[2:], mode='bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Measure accuracy and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T04:55:47.570430Z",
     "start_time": "2021-06-18T04:55:47.559718Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image # import \n",
    "def visualization(net, image, epoch, device):\n",
    "    net.to('cpu')\n",
    "    mean_bgr = np.array([104.00698793, 116.66876762, 122.67891434])\n",
    "    img = image\n",
    "    img = np.array(img, dtype = np.uint8)\n",
    "    img = img[:, :, ::-1] # channel RGB -> BGR\n",
    "    img = img.astype(np.float64)\n",
    "    img -= mean_bgr\n",
    "    img = img.transpose(2, 0, 1) # H W C -> C H W\n",
    "    img = torch.from_numpy(img).float()\n",
    "    img = img.unsqueeze(0)\n",
    "\n",
    "    score = net(img) # 네트워크의 output(logits), B x C x H x W\n",
    "    lbl_pred = score.data.max(1)[1].cpu().numpy() # [max value, idx]     (1) -> B x 1(max인 index 0 또는 1) x H x W\n",
    "    lbl_pred = np.squeeze(lbl_pred)\n",
    "\n",
    "    print(score.data.max(1))\n",
    "\n",
    "    Image.fromarray((lbl_pred * 255).astype(np.uint8)).save('./resources/pred/mask_'+str(epoch+1)+'.png')\n",
    "    \n",
    "    input_img = image\n",
    "    input_img = np.array(input_img, dtype = np.uint8)\n",
    "    color = [0, 255, 0, 127] \n",
    "    color = np.array(color).reshape(1, 4)\n",
    "    shape = input_img.shape\n",
    "    segmentation = lbl_pred.reshape(shape[0], shape[1], 1)\n",
    "    output = np.dot(segmentation, color)\n",
    "\n",
    "    output = Image.fromarray(output.astype(np.uint8))\n",
    "    background = Image.fromarray(input_img.astype(np.uint8))\n",
    "    background.paste(output, box = None, mask = output)\n",
    "    background.save('./resources/overlay/overlay_'+str(epoch+1)+'.png')\n",
    "\n",
    "    net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-18T04:55:49.931Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch : 0\n",
      "torch.Size([1, 512, 30, 138])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piai/anaconda3/envs/abc/lib/python3.8/site-packages/torch/nn/functional.py:2952: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/piai/anaconda3/envs/abc/lib/python3.8/site-packages/torch/nn/functional.py:3060: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\"Default upsampling behavior when mode={} is changed \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "current epoch : 1\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 136])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n",
      "torch.Size([1, 512, 30, 138])\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model_type = 'unet'\n",
    "\n",
    "if model_type == 'unet':\n",
    "    net = UNet(2)\n",
    "elif model_type == 'FCN':\n",
    "    net = FCN(num_class = 2)\n",
    "    vgg16 = torchvision.models.vgg16(pretrained = True)\n",
    "    net.copy_params(vgg16)\n",
    "    del vgg16\n",
    "\n",
    "    \n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = net.to(device)\n",
    "\n",
    "training_epochs = 10 \n",
    "\n",
    "# TODO\n",
    "# criterion : Cross entropy loss\n",
    "# optimizer : Adam, learning weight=0.004\n",
    "criterion = nn.CrossEntropyLoss(size_average=False) # size_average : 배치에 각 이미지의 loss를 average 하느냐 마느냐.\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4)\n",
    "\n",
    "num_class = 2\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    print ('current epoch : %d'%(epoch))\n",
    "    # training\n",
    "    net.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # TODO\n",
    "        # data to gpu\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # forward\n",
    "        score = net(data)\n",
    "        \n",
    "        # loss = ...\n",
    "        loss = criterion(score, target)\n",
    "        \n",
    "        # update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         if batch_idx % 20 ==0:\n",
    "#             print ('batch : %d, loss : %f'%(batch_idx, loss.item()))\n",
    "        \n",
    "    #validation\n",
    "#     net.eval()\n",
    "    \n",
    "#     val_loss = 0\n",
    "\n",
    "#     for batch_idx, (data, target) in enumerate(val_loader):\n",
    "#         # TODO\n",
    "#         # load data\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "#         # forward\n",
    "#         score = net(data)\n",
    "        \n",
    "#         loss = criterion(score, target)\n",
    "#         val_loss += loss.item() / len(data)\n",
    "            \n",
    "#     val_loss /= len(val_loader)\n",
    "#     print ('val loss : %f'%val_loss)\n",
    "        \n",
    "#     #visualization\n",
    "#     img = PIL.Image.open('./resources/Kitti/data_road/testing/image_2/um_000081.png')\n",
    "#     visualization(net, img, epoch, device)\n",
    "    \n",
    "        \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
