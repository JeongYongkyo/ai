{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13fd00f4",
   "metadata": {},
   "source": [
    "# Deep Q-learning\n",
    "Refer to the lecture note `8DQN.pdf`\n",
    "\n",
    "## To run jupyter without remote redering (e.g. on a local machine), execute jupyter by:\n",
    "`jupyter notebook --ip=YOUR_IP --notebook-dir YOUR_WORKING_DIR`\n",
    "\n",
    "## To run jupyter with remote redering (e.g. on a remote machine), execute jupyter by:\n",
    "`xvfb-run -s \"-screen 0 600x400x24\" jupyter notebook --ip=YOUR_IP --notebook-dir YOUR_WORKING_DIR`\n",
    "\n",
    "## Important argument\n",
    "`do_render`: set ```True``` to render (visualize) the environment on the notebook for each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b3040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN 단점\n",
    "# action이 continuos인 경우 어렵다\n",
    "# epsilon greedy 사용 일정 수준의 확률로 탐험 -> epsilon에 영향을 받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcc54bae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T01:11:07.123043Z",
     "start_time": "2021-07-01T01:11:05.781321Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ac00367",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T01:11:07.133375Z",
     "start_time": "2021-07-01T01:11:07.124749Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([[2], [3]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = deque(maxlen=2)\n",
    "l.append([1])\n",
    "l.append([2])\n",
    "l.append([3])\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c849b9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T01:11:07.315565Z",
     "start_time": "2021-07-01T01:11:07.135285Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b59fbe10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T02:13:09.929271Z",
     "start_time": "2021-07-01T02:13:09.923308Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "gamma = 0.95\n",
    "lr = 0.005 # 0.001\n",
    "batch_size = 32\n",
    "num_replay = 10\n",
    "max_replay_memory = 10000\n",
    "\n",
    "eps = 1.0\n",
    "eps_decay = 0.995\n",
    "eps_min = 0.1\n",
    "\n",
    "do_render = False\n",
    "print_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5309a735",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T02:13:10.969042Z",
     "start_time": "2021-07-01T02:13:10.963085Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_state(env, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(info)\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8999a916",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T01:11:07.377155Z",
     "start_time": "2021-07-01T01:11:07.346156Z"
    }
   },
   "outputs": [],
   "source": [
    "# 경험 저장\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=max_replay_memory): # max_replay_memory만큼 저장하고 오래된 거는 버린다.\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    # 매 스텝마다 (s, a, r, s', done) append\n",
    "    def put(self, state, action, reward, next_state, done):\n",
    "        transition = [state, action, reward, next_state, done]\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self):\n",
    "        sample_size = min(batch_size, self.size()) # batch_size보다 작으면 buffer의 size를 sample로 사용\n",
    "        sample = random.sample(self.buffer, sample_size)\n",
    "        states, actions, rewards, next_states, done = map(np.asarray, zip(*sample)) # sample에있는 element를 ndarray로 바꿀수 있는 형태면 ndarray 바꿈\n",
    "        #print(states.__class__, actions.__class__)\n",
    "        \n",
    "        states = np.array(states).reshape(sample_size, -1)\n",
    "        next_states = np.array(next_states).reshape(sample_size, -1)\n",
    "        return states, actions, rewards, next_states, done\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccd90dc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T01:11:07.403320Z",
     "start_time": "2021-07-01T01:11:07.379761Z"
    }
   },
   "outputs": [],
   "source": [
    "# q function\n",
    "class ActionStateModel(tf.keras.Model):\n",
    "    def __init__(self, state_dim, aciton_dim):\n",
    "        super(ActionStateModel, self).__init__()\n",
    "        self.state_dim  = state_dim\n",
    "        self.action_dim = aciton_dim\n",
    "        self.epsilon = eps\n",
    "        \n",
    "        self.net = self.create_model()\n",
    "    \n",
    "    # tf.keras.Model은 무조건 이 함수를 정의해야함\n",
    "    def call(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def create_model(self):\n",
    "        # Q(S, a)\n",
    "        # net(s) -> a 개수만큼의 q value\n",
    "        # net(s)[0]\n",
    "        # 한 state에서 할 수 있는 모든 action에 대한 q value\n",
    "        model = tf.keras.Sequential([\n",
    "            Input((self.state_dim,)),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(self.action_dim) # output이 action개수만큼 나옴\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    # 점차 줄어드는 epsilon 구현\n",
    "    def update_eps(self):\n",
    "        self.epsilon *= eps_decay # 한 번 감쇄시켜준다.\n",
    "        self.epsilon = max(self.epsilon, eps_min) # 큰거 선택\n",
    "\n",
    "    # eps greedy\n",
    "    def get_action(self, state): # (state_dim -> int)\n",
    "        self.update_eps()\n",
    "\n",
    "        # np.random.random() : [0.0, 1.0] range안에서 uniform 분포따르게 random 선택\n",
    "        # 탐색\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1) # 지금 epsilon보다 작으면 아무거나 선택하겠다. (왼쪽 or 오른쪽)\n",
    "        else: # epsilon보다 크다면 정착\n",
    "            # (input_dim)\n",
    "            # (batch, input_dim) -> (1, input_dim)\n",
    "            state = state[np.newaxis, ...] # 가짜 batch 차원에 1을 넣어줌, ...은 그대로 두고 앞에 가짜 dim넣는다.\n",
    "            q_value = self.predict(state)[0] # 배치가 1이라서 indexing함\n",
    "            return np.argmax(q_value) # greedy selection, 가장 큰값의 index를 뽑는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0a7334",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T01:59:32.467818Z",
     "start_time": "2021-07-01T01:26:27.360Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.arange(9).reshape(3,3)\n",
    "print(a.shape)\n",
    "print(ap[np.newaxis,...].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee4f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.randint(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8666ea8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T02:14:07.429376Z",
     "start_time": "2021-07-01T02:14:07.411141Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 학습의 전반적인 관리\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        # open ai gym에서 정의되었음\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        \n",
    "        # 최근 10000개만 저장\n",
    "        self.buffer = ReplayBuffer()  # D\n",
    "\n",
    "        # 두 개의 모델 초기화\n",
    "        # model이 target_model을 따라가도록\n",
    "        # model : \\theta\n",
    "        # target_model : \\theta-\n",
    "        self.model = ActionStateModel(self.state_dim, self.action_dim)\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr)) # regression\n",
    "        self.target_model = ActionStateModel(self.state_dim, self.action_dim)\n",
    "        self.target_model.compile(loss='mse', optimizer=Adam(lr)) # gradient를 update안하고 항상 weight를 복사해서 사용\n",
    "        self.target_update()       \n",
    "\n",
    "    def target_update(self):\n",
    "        # model의 weight를 복사해서 target_model에 넣어줌(두 모델의 network가 같아서 가능)\n",
    "        # target의 변화도가 너무 큼 (해결책: 99% 지금 알고있는 것 사용 1% 바꾼다.)\n",
    "        weights = self.model.get_weights()\n",
    "        self.target_model.set_weights(weights)\n",
    "    \n",
    "    def replay(self):\n",
    "        # num_replay번 sampling하고 update\n",
    "        for _ in range(num_replay):\n",
    "            # batch_size만큼 (s, a, r, s')\n",
    "            states, actions, rewards, next_states, done = self.buffer.sample()\n",
    "            \n",
    "            y = self.target_model.predict(states) # state -> Q\n",
    "            # 처음에는 self.model.predict(states)와 같다.\n",
    "            print(y.shape) # (batch_size, num_actions)  (32, 2)\n",
    "            \n",
    "            next_q_values = self.target_model.predict(next_states).max(axis=1) # max를 가져옴\n",
    "            y[range(y.shape[0]), actions] = rewards + (1 - done) * next_q_values * gamma # 게임이 끝나면 done이 1이되고 reward만 가져오면 된다.\n",
    "            # 각 state에서 action에 대한 reward가 수정된 부분만 update 반영하겠다. (action이 0이면 0번 column, 1이면 1번 column)\n",
    "            print(y[:5])\n",
    "            print(self.model.predict(states)[:5])\n",
    "            # forward/backward 일어나는 부분\n",
    "            self.model.fit(states, y, verbose=0)\n",
    "    \n",
    "    def train(self, max_episodes=1000):\n",
    "        total_reward = []\n",
    "\n",
    "        for ep in range(max_episodes):\n",
    "            done, running_reward = False, 0\n",
    "            state = self.env.reset() # 게임 끝나면 다시 초기화\n",
    "\n",
    "            while not done: # done=True가 될 때까지 반복\n",
    "                action = self.model.get_action(state) # 0, 1\n",
    "                # cart-pole : reward 1초에 1\n",
    "                # state : 4dim vector\n",
    "                next_state, reward, done, _ = self.env.step(action)  # reward always 1.0\n",
    "                # replay buffer에 넣어준다.\n",
    "                self.buffer.put(state, action, reward, next_state, done)\n",
    "                running_reward += reward\n",
    "                state = next_state # 다음 state를 지금 state로 변경\n",
    "                if do_render:\n",
    "#                     self.env.render()  # if you are running on a local machine\n",
    "                    show_state(self.env, info=f'episode: {ep} | episode reward={running_reward}')\n",
    "            \n",
    "            # 원래는 1step당 update해야하는데 1episode당 update로 수정\n",
    "            total_reward.append(running_reward) # print용\n",
    "            self.replay() # 옛날의 경험을 바탕으로 replay\n",
    "            self.target_update()\n",
    "        \n",
    "            if ep % print_freq == 0:\n",
    "                print(f'episode: {ep:>3} | avg. reward: {np.mean(total_reward[-print_freq:])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5c150ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T01:58:04.728119Z",
     "start_time": "2021-07-01T01:11:07.438335Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:   0 | avg. reward: 25.0\n",
      "episode:  10 | avg. reward: 17.2\n",
      "episode:  20 | avg. reward: 39.7\n",
      "episode:  30 | avg. reward: 49.0\n",
      "episode:  40 | avg. reward: 57.3\n",
      "episode:  50 | avg. reward: 99.2\n",
      "episode:  60 | avg. reward: 119.1\n",
      "episode:  70 | avg. reward: 100.6\n",
      "episode:  80 | avg. reward: 176.4\n",
      "episode:  90 | avg. reward: 187.1\n",
      "episode: 100 | avg. reward: 209.5\n",
      "episode: 110 | avg. reward: 320.9\n",
      "episode: 120 | avg. reward: 287.0\n",
      "episode: 130 | avg. reward: 364.2\n",
      "episode: 140 | avg. reward: 405.2\n",
      "episode: 150 | avg. reward: 274.4\n",
      "episode: 160 | avg. reward: 446.6\n",
      "episode: 170 | avg. reward: 437.7\n",
      "episode: 180 | avg. reward: 406.8\n",
      "episode: 190 | avg. reward: 409.2\n",
      "episode: 200 | avg. reward: 358.9\n",
      "episode: 210 | avg. reward: 366.3\n",
      "episode: 220 | avg. reward: 421.2\n",
      "episode: 230 | avg. reward: 380.1\n",
      "episode: 240 | avg. reward: 357.1\n",
      "episode: 250 | avg. reward: 279.7\n",
      "episode: 260 | avg. reward: 274.8\n",
      "episode: 270 | avg. reward: 318.0\n",
      "episode: 280 | avg. reward: 316.2\n",
      "episode: 290 | avg. reward: 316.9\n",
      "episode: 300 | avg. reward: 312.4\n",
      "episode: 310 | avg. reward: 261.0\n",
      "episode: 320 | avg. reward: 300.4\n",
      "episode: 330 | avg. reward: 309.1\n",
      "episode: 340 | avg. reward: 268.7\n",
      "episode: 350 | avg. reward: 296.5\n",
      "episode: 360 | avg. reward: 303.7\n",
      "episode: 370 | avg. reward: 383.4\n",
      "episode: 380 | avg. reward: 339.6\n",
      "episode: 390 | avg. reward: 341.4\n",
      "episode: 400 | avg. reward: 294.1\n",
      "episode: 410 | avg. reward: 277.5\n",
      "episode: 420 | avg. reward: 310.7\n",
      "episode: 430 | avg. reward: 256.5\n",
      "episode: 440 | avg. reward: 232.3\n",
      "episode: 450 | avg. reward: 239.6\n",
      "episode: 460 | avg. reward: 293.8\n",
      "episode: 470 | avg. reward: 246.2\n",
      "episode: 480 | avg. reward: 259.4\n",
      "episode: 490 | avg. reward: 275.5\n"
     ]
    }
   ],
   "source": [
    "# 오뚜기 중심잡기 게임\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('MountainCar-v0')\n",
    "agent = Agent(env)\n",
    "agent.train(max_episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cd22d2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-01T02:04:35.605046Z",
     "start_time": "2021-07-01T02:00:15.598984Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAAD3CAYAAABCbaxBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOKElEQVR4nO3ce9BcZX3A8e8PCQRQWu5puAQEKRRaS0UZGCmZ8UYEbBtmvBQQrFPhD0sZmdHqFA0qHVpsa4Y2QqmFKVLLRRouU+ulJRGkGNNMqw0FRRJIgAAhBHi5hSRP/3iehcP67mYTfviS5PuZ2Zl995x9zuV997tnz9kkSilIkl65bSZ6BSRpS2FQJSmJQZWkJAZVkpIYVElKYlAlKYlBTRARiyNievKYV0TEFzPHHHG5+0fE0ldp7Esi4rzkMc+IiNsyx5xoETE9IpZP9Hpo4xnUBKWUw0op8yZ6PQaJiGMiYkFEPBURP4qIt0/EepRSziqlfGEilr2li4jtI+KrEXFf+z3/d0TM6EzfLiKui4ilEVH6DwAi4psRMda5rYmIHw9Z3jsi4q6IeCYibomIaa/e1m0+DOoWLiJ2BW4CLgJ+GfgL4KaI2GUi1+u1Jqpf+OshIrZNGmpbYBlwHPBLwJ8C10TE/p15bgNOBVb0P7mUMqOU8vreDbgduHbAOu8OXA+cB+wKLASuTtqOzZpBbSJiakR8IyIejYglEXF2Z9qs9u5+dXv3XxQRb+5MXxoR72z33xYRCyPiyYh4OCL+qjPf+9rpgdURMS8iDu1MO6KN+1REXA1M7lu/E9tRx+qIuD0ifmPETTsGWFFKubaUsq6U8jXgUWDmJu6n7SPiSxFxf9u+SyJihzZtekQsj4jPRMTKtl9O6Tz3xdMYEbF7RNzctmdVRNzaC1pEHNr2z+q2v97XGWO3iLix7d8FwIF963dIRHynjXl3RLx/yLbMi4gLIuL7wDPAGwc9PyIOaOvTW8fLIuKRzlhXRsQ57f5HIuL/2u/y3og4szNfbx99KiJWAJdHxA5t3zweEXcCb93Y30sp5elSyqxSytJSyvpSys3AEuAtbfqaUsqXSym3AeuGjdUifCzwjwNmmQksbn9TzwGzgDdHxCEbu95bGoMKtBfJTcD/AHsD7wDOiYj3dGb7Heo79q7APwFzI2LSOMPNBmaXUnamvtivacs4GPg6cA6wB/Cv1CPF7SJiO2AucGUb/1rg5M76HQH8A3AmsBtwKXBjRGzfps+JiDnDNnGcnw8fMv8wFwIHA78JHETdX5/tTJ8C7N4ePx34u4j41XHGORdYTt0XewGfAUrbpzcB3wb2BP4IuKozxt8CzwG/AvxBu9WNitgJ+A7197Mn8EFgTkT82pDtOQ34GPAG6hvNuM8vpSwBngSOaM/7bWCs86Z4HDC/3X8EOBHYGfgI8NcR8Vt9+2hXYFpb9ueofysHAu9p++1FnTee8W43j7dREbEX9fe0eMi2D/Jh4NZSytIB0w+jvlaAGnPgZ+3xrVspZau/AUcB9/c99mng8nZ/FnBHZ9o2wEPAse3npcA72/3vAecDu/eNdx5wTd8YDwDTqS/OB4HoTL8d+GK7/xXgC33j3Q0cN8K27QasBj4ETKK+WNcDlw6Yf39g6YBpATwNHNh57GhgSbs/HVgL7NSZfg1wXrt/RWebPg/cABzUt4xjqR9Jt+k89vX2O3gd8AJwSGfanwG3tfsfoIagO96lwOcGbM884POdn4c+n/qG9wlqEO+mnj45Czig7eNtBixnLvDHnX20BpjcmX4vcHzn548By1/B3/Mk4LtDfsfLgelDnn8PcMaQ6V8FLux77PvDnrO13DxCraYBU7vv/NQjpr068yzr3SmlrKf+UU4dZ6yPUo8M7oqIH0bEie3xqcB9fWMsox7JTQUeKO0vs7mvc38acG7f+u07YPkvU0p5jHp0/QngYeB46ottU64i7wHsCPxXZz3+rT3e83ipRyzd7RhvPS+ivnC/3T4W/0l7fCqwrO2f7hh7t+X0zhV2p/VMA47q20+nUAM4SHesDT1/Pi+9AX6PGuTj2u3W3jpHxIyIuKOdNlgNvJd61N7zaKkflXumDtmmjdI+bV1JjfbHN+H5b6du73VDZhujHn137Qw8tbHL29JknRDf3C2jHmW9acg8+/butD/afahHlS9TSvkp8KE2z0zguojYrc37650xoo35AFCAvSMiOlHdj/oxqrd+F5RSLtiUjSulzKedl4t6EeRe4C83YaiVwLPAYaWUBwbMs0tE7NSJ6n7A/46zTk9RP/afGxGHA/8RET+k7qd9I2KbTlT3A35C/Ui+lrrf7upM61kGzC+lvGsjtqn7Jrah58+nvhEsb/dvAy6hnoKYD/UcM/AN6sfmG0opL0TEXF5+2qX/v3h7qG1T7+N5d5uIiG9Sj9zHc2spZUabL6hHj3sB7y2lvDDgOcOcDlxfShkbMs9iOqcl2qmWA9m00wtbFI9QqwXAU+1CwQ4R8bqIODwiuhcH3hIRM1uQzgGeB+7oHygiTo2IPVoMVreH11M/+p4Q9esmk6gxeZ760f4/qaE4OyImRcRM4G2dYS8DzoqIo6LaKSJOiIg3jLJxUS94TYqInYEvUY8AvzXqzulp23QZ9Zzgnm3svfvONQOc384NH0s9l/hzV4ujXmQ7qEXgCeqFkvXAD6gXiD7Z1nk6cBLwz6WUddSry7MiYsd2brR7vvFm4OCIOK09d1JEvLVznnNDhj6/vVk+S71SPr+U8iT1qP9kXjp/uh2wPS3+Ub+69O4NLPca4NMRsUtE7EM9b/yi0ncFvu82ozPrV4BDgZNKKc/2LyTqBcXexc7tImJy2/+96TsA76eemhnmX4DDI+LkNt5ngR+VUu7awPO2eAYVaC/UE6kXWpZQj8T+nvr1k54bqOfYHqdeyJg54AjgeGBxRIxRL1B9sJTybCnlbuoL8eI2/knUP/w1pZQ11KPZM4BVbTnXd9ZvIfCHwN+05d/T5gVe/ML8JUM28ZNtmcuoF3N+b0P7ZIhPteXfERFPUk8fdC86rWjr+CBwFXDWgBfam9pzx6hvKHNKKbe0fXESMKOt8xzgw50xPg68vi3nCuDy3oDtqPfd1ItJD7Z5/pwauA0a8fnzgcdKKcs6PwewqDPG2dRIPg78PnDjBhZ9PvVj/hLqxbgrR1nfrqjfAz2T+je8Il76Pukpndnupr4h7A18q93vfn/0d6kHAbeMM/7i3lillEepbyIXULfxKOo+2+rFy0/baTwRMYt68eTUiV6XV1vUr8zMK6XsvwnPnQ58rZSyT+5aSZsHj1AlKYlBVb/VwJcneB2kzZIf+SUpiUeokpRkQ99D9fBVkn5e/z/nBjxClaQ0BlWSkhhUSUpiUCUpiUGVpCQGVZKSGFRJSmJQJSmJQZWkJAZVkpIYVElKYlAlKYlBlaQkBlWSkhhUSUpiUCUpiUGVpCQGVZKSGFRJSmJQJSmJQZWkJAZVkpIYVElKYlAlKYlBlaQkBlWSkhhUSUpiUCUpiUGVpCQGVZKSGFRJSmJQJSmJQZWkJAZVkpIYVElKYlAlKYlBlaQkBlWSkhhUSUpiUCUpiUGVpCQGVZKSGFRJSmJQJSmJQZWkJAZVkpIYVElKYlAlKYlBlaQkBlWSkhhUSUpiUCUpiUGVpCQGVZKSGFRJSmJQJSmJQZWkJAZVkpIYVElKYlAlKYlBlaQkBlWSkhhUSUpiUCUpiUGVpCQGVZKSGFRJSmJQJSmJQZWkJAZVkpIYVElKYlAlKYlBlaQkBlWSkhhUSUpiUCUpiUGVpCQGVZKSGFRJSmJQJSmJQZWkJAZVkpIYVElKYlAlKYlBlaQkBlWSkhhUSUpiUCUpiUGVpCQGVZKSGFRJSmJQJSmJQZWkJAZVkpIYVElKYlAlKYlBlaQkBlWSkhhUSUpiUCUpiUGVpCQGVZKSGFRJSmJQJSmJQZWkJAZVkpIYVEnaCKvuWTBwmkGVpI3wzMr7B04zqJKUxKBKUhKDKklJDKokJTGokpTEoEpSEoMqSUkMqiQlMaiSlMSgSlISgypJIyrr17FuzbMDpxtUSRrRmqdX89hPfzBwukGVpJEVKOsHTjWokpTEoEpSEoMqSUkMqiQlMaiSlMSgSlISgypJSQyqJCUxqJKUxKBKUhKDKkkjeuK+H1P8p6eS9MqNPfwzKGXgdIMqSUkMqiQlMaiSlMSgSlISgypJSQyqJCUxqJKUxKBKUhKDKklJDKokJTGokjSCNWOreGblfUPnMaiSNIK1z43x/BOPDJ3HoEpSEoMqSUkMqiQlMaiSlMSgSlISgypJSQyqJCUxqJKUxKBKUpJtJ3oFJGmizJ49m3nz5o0075Sdt+Wjx+xCRAycx6BK2motXLiQuXPnjjTvEQdN4fSjT2b9+sHZNKiSNIIPvOto7lh1AmNrd+GYAfMYVEkawaq1b2THF/YABn/k96KUJI3goecOYFhMwaBK0kim7XgnUIbOY1AlaQTrnv4Ja1YvYuXKpQPn8RyqJI3gwqu+C/w7EFx88bpx5xka1IsuuuhVWC1Jem248847R563FKgf+Qd/7B8a1NNOO23khUnS5mbBggUsWrQobbyhQZ0yZUragiTptWby5Mmp43lRSpKSGFRJSmJQJSmJQZWkJAZVkpL4xX5JW60jjzySsbGxtPGilKH/NnX4P1yVpK3TuP9Lih/5JSmJQZWkJAZVkpIYVElKYlAlKYlBlaQkBlWSkhhUSUpiUCUpiUGVpCQGVZKSGFRJSmJQJSmJQZWkJAZVkpIYVElKYlAlKYlBlaQkBlWSkhhUSUpiUCUpiUGVpCQGVZKSGFRJSmJQJSmJQZWkJAZVkpIYVElKYlAlKYlBlaQkBlWSkhhUSUpiUCUpiUGVpCQGVZKSGFRJSmJQJSmJQZWkJAZVkpIYVElKYlAlKYlBlaQkBlWSkhhUSUpiUCUpybYbmB6/kLWQpC2AR6iSlMSgSlISgypJSQyqJCUxqJKUxKBKUpL/B9/CFBlrBm2sAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.train(max_episodes=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
