{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<font size='6'><b>Markov Decision Process</b></font><br><br>\n",
    "\n",
    "<br>\n",
    "<div class=pull-right>\n",
    "By Prof. Seungchul Lee<br>\n",
    "http://iai.postech.ac.kr/<br>\n",
    "Industrial AI Lab at POSTECH\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source\n",
    "\n",
    "- By David Silver's RL Course at UCL\n",
    "- By Prof. Zico Kolter at CMU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Table of Contents\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Markov Decision Process\n",
    "\n",
    "- So far, we analyzed the passive behavior of a Markov chain with rewards\n",
    "\n",
    "\n",
    "- A Markov decision process (MDP) is a Markov reward process with decisions (or actions). \n",
    "\n",
    "\n",
    "## 1.1. MDP Definition\n",
    "\n",
    "\n",
    "Definition: A Markov Decision Process is a tuple $\\langle S,\\color{red}{A},P,R,\\gamma \\rangle$\n",
    "\n",
    "- $S$ is a finite set of states\n",
    "- <font color='red'>$A$ is a finite set of actions</font>\n",
    "- $P$ is a state transition probability matrix\n",
    "\n",
    "$$P_{ss'}^\\color{red}{a} = P\\left[S_{t+1} = s' \\mid S_t = s, A_t = \\color{red}{a} \\right]$$\n",
    "\n",
    "- $R$ is a reward function, $R_s^\\color{red}{a} = \\mathbb{E} \\left[ R_{t+1} \\mid S_t = s, A_t = \\color{red}{a} \\right] \\quad \\left(= \\mathbb{E} \\left[ R_{t+1} \\mid S_t = s \\right], \\; \\text{often assumed} \\right)$\n",
    "- $\\gamma$ is a discount factor, $\\gamma \\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Startup MDP\n",
    "\n",
    "- You run a startup company. In every state, you must choose between Saving money or Advertising\n",
    "\n",
    "<br>    \n",
    "<img src = \"./image_files/MDP_SA_example.png\" width = 500> \n",
    "\n",
    "\n",
    "## 1.2. Policies\n",
    "\n",
    "- A policy is a mapping from state to actions, $\\pi: S \\rightarrow A$\n",
    "\n",
    "\n",
    "- A policy fully defines the behavior of an agent\n",
    "    - It can be deteministic or stochastic\n",
    "\n",
    "\n",
    "- Given a state, it specifies a distribution over actions\n",
    "\n",
    "\n",
    "$$\\pi (a \\mid s) = P(A_t = a \\mid S_t = s)$$\n",
    "\n",
    "- MDP policies depend on the current state (not the history)\n",
    "\n",
    "\n",
    "- Policies are stationary (time-independent, but it turns out to be optimal)\n",
    "\n",
    "\n",
    "- Let $P^{\\pi}$ be a matrix containing probabilities for each transition under policy $\\pi$\n",
    "\n",
    "\n",
    "- Given an MDP $\\mathcal{M} = \\langle S,A,P,R,\\gamma \\rangle$ and a policy $\\pi$\n",
    "\n",
    "    - The state sequence $s_1, s_2, \\cdots $ is a Markov process $\\langle S, P^{\\pi} \\rangle$\n",
    "\n",
    "    - The state and reward sequence is a Markov reward process $\\langle S,P^{\\pi},R^{\\pi},\\gamma \\rangle$\n",
    "\n",
    "\n",
    "- Questions on MDP policy\n",
    "    - How many possible policies in our example?\n",
    "\n",
    "    - Which of the above two policies is best?\n",
    "\n",
    "    - How do you compute the optimal policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bellman Equation\n",
    "\n",
    "\n",
    "## 2.1. Value Function\n",
    "\n",
    "__State-value function__ \n",
    "\n",
    "- The state-value function $v_{\\pi}(s)$ of an MDP is the exptected return starting from state $s$, and then following policy $\\pi$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &= \\mathbb{E}_{\\pi} \\left[G_t \\mid S_t =s \\right]\\\\\n",
    "&= \\mathbb{E}_{\\pi}[ R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) \\mid S_t = s ]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "__Action-value function__\n",
    "\n",
    "- The action-value function $q_{\\pi}(s,a)$ of an MDP is the exptected return starting from state $s$, taking action $a$, and then following policy $\\pi$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s,a) &= \\mathbb{E}_{\\pi} \\left[G_t \\mid S_t =s, A_t = a \\right]\\\\\n",
    "& = \\mathbb{E}_{\\pi} \\left[R_{t+1} + \\gamma q_{\\pi} \\left(S_{t+1}, A_{t+1} \\right) \\mid S_t =s, A_t = a \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Bellman <font color='red'>Expectation</font> Equation\n",
    "\n",
    "\n",
    "\n",
    "### 2.2.1. Relationship between $v_{\\pi}(s)$ and $q_{\\pi}(s,a)$\n",
    "\n",
    "- State-value function using policy $\\pi$\n",
    "\n",
    "<br>\n",
    " \n",
    "$$v_{\\pi}(s) = \\sum_{a \\in A}\\pi(a \\mid s) q_{\\pi}(s,a)$$\n",
    "\n",
    "<br>\n",
    "<img src = \"./image_files/BellmanExpV.png\" width = 250> \n",
    "\n",
    "\n",
    "- Action-value function using policy $\\pi$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$q_{\\pi}(s,a) =  R(s) + \\gamma \\sum_{s' \\in S} P\\left(s' \\mid s,a \\right) v_{\\pi}\\left(s'\\right)$$\n",
    "\n",
    "<br>\n",
    "<img src = \"./image_files/BellmanExpQ.png\" width = 250> \n",
    "\n",
    "### 2.2.2. Bellman Expectation Equation\n",
    "\n",
    "- Bellman Expectation Equation for $v_{\\pi}(s)$\n",
    "\n",
    "<br>\n",
    "$$\n",
    "\\begin{align*}\n",
    "v_{\\pi}(s) &= \\sum_{a \\in A}\\pi(a \\mid s) \\underline{q_{\\pi}(s,a)} \\\\ \\\\ \n",
    " &= \\sum_{a \\in A}\\pi(a \\mid s) \\left( R(s) + \\gamma \\sum_{s' \\in S} P\\left(s' \\mid s,a \\right) v_{\\pi}\\left(s'\\right)  \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<img src = \"./image_files/BellmanExpQQ.png\" width = 300> \n",
    "\n",
    "\n",
    "- Bellman Expectation Equation for $q_{\\pi}(s,a)$\n",
    "\n",
    "<br>\n",
    "$$\n",
    "\\begin{align*}\n",
    "q_{\\pi}(s,a) &=  R(s) + \\gamma \\sum_{s' \\in S} P\\left(s' \\mid s,a \\right) \\underline{v_{\\pi} \\left(s'\\right)}\\\\ \\\\ \n",
    " &=  R(s) + \\gamma \\sum_{s' \\in S} P\\left(s' \\mid s,a \\right) \\left( \\sum_{a' \\in A} \\pi \\left(a' \\mid s' \\right) q_{\\pi} \\left(s', a' \\right) \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<img src = \"./image_files/BellmanExpQQ.png\" width = 300> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Solving the Bellman Expectation Equation\n",
    "\n",
    "- The Bellman expectation equation can be expressed concisely in a matrix form,\n",
    "\n",
    "<br>\n",
    "\n",
    "$$v_{\\pi} = R + \\gamma  \\, P^{\\pi} \\,v_{\\pi}  \\quad \\implies \\quad v_{\\pi} = (I - \\gamma  \\,P^{\\pi})^{-1}R $$\n",
    "\n",
    "<br>\n",
    "\n",
    "- Iterative\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ v_{\\pi} (s) \\; \\leftarrow \\; R(s) + \\gamma \\sum_{s' \\in S} P\\left(s' \\mid s, a \\right) \\;v_{\\pi} \\left(s' \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example__\n",
    "\n",
    "- Given the policy 1, it is MRP\n",
    "\n",
    "<img src = \"./image_files/MDP_policy_01.png\" width = 700> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T05:06:20.738554Z",
     "start_time": "2021-04-29T05:06:20.595404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        ]\n",
      " [ 0.        ]\n",
      " [18.18181818]\n",
      " [10.        ]]\n"
     ]
    }
   ],
   "source": [
    "# [PU PF RU RF] = [0 1 2 3]\n",
    "# MDP문제이지만 policy가 있기때문에 MRP로 봐도 된다.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "P = [[1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0.5, 0, 0.5, 0],\n",
    "    [0, 1, 0, 0]]\n",
    "\n",
    "R = [0, 0, 10, 10]\n",
    "\n",
    "P = np.asmatrix(P)\n",
    "R = np.asmatrix(R)\n",
    "R = R.T\n",
    "\n",
    "gamma = 0.9\n",
    "v = (np.eye(4) - gamma*P).I*R\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T05:06:34.544397Z",
     "start_time": "2021-04-29T05:06:34.532980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        ]\n",
      " [ 0.        ]\n",
      " [18.18181818]\n",
      " [10.        ]]\n"
     ]
    }
   ],
   "source": [
    "# calculate value using iteration, gamma = 0.9\n",
    "v = np.zeros([4,1])\n",
    "\n",
    "for i in range(100):\n",
    "    v = R + gamma*P*v\n",
    "\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given the policy 2, it is MRP\n",
    "\n",
    "<img src = \"./image_files/MDP_policy_02.png\" width = 700> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T05:06:56.059837Z",
     "start_time": "2021-04-29T05:06:56.047680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 0.]\n",
      " [10.]\n",
      " [10.]]\n"
     ]
    }
   ],
   "source": [
    "# [PU PF RU RF] = [0 1 2 3]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "P = [[0.5, 0.5, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0.5, 0.5, 0, 0],\n",
    "    [0, 1, 0, 0]]\n",
    "\n",
    "R = [0, 0, 10, 10]\n",
    "\n",
    "P = np.asmatrix(P)\n",
    "R = np.asmatrix(R)\n",
    "R = R.T\n",
    "\n",
    "gamma = 0.9\n",
    "v = (np.eye(4) - gamma*P).I*R\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T05:06:56.513317Z",
     "start_time": "2021-04-29T05:06:56.502380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 0.]\n",
      " [10.]\n",
      " [10.]]\n"
     ]
    }
   ],
   "source": [
    "# calculate value using iteration, gamma = 0.9\n",
    "v = np.zeros([4,1])\n",
    "\n",
    "for i in range(100):\n",
    "    v = R + gamma*P*v\n",
    "\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Bellman <font color='red'>Optimality</font> Equation\n",
    "\n",
    "\n",
    "\n",
    "### 2.3.1. Optimal Value Function\n",
    "\n",
    "- The optimal state-value functin $v_*(s)$ is the maximum value function over all policies\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "v_*(s) &= \\max_{\\pi} v_{\\pi}(s) \\\\\n",
    "& = \\max_a q_{\\pi}(s,a) \\\\\n",
    "& = \\max_a \\left( R(s) + \\gamma \\sum_{s' \\in S} P(s' \\mid s,a) v_{\\pi}(s') \\right)\\\\\n",
    "& = R(s) + \\gamma \\max_a \\sum_{s' \\in S} P(s' \\mid s,a) v_{\\pi}(s')\n",
    "\\end{align*}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "- The optimal action-value functin $q_*(s,a)$ is the maximum action-value function over all policies\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "q_*(s,a) &= \\max_{\\pi} q_{\\pi}(s,a) \\\\\n",
    "& =  \\max_{\\pi} \\left( R(s) + \\gamma \\sum_{s' \\in S} P(s' \\mid s,a) v_{\\pi}(s') \\right) \\\\\n",
    "& = R(s) + \\gamma \\sum_{s' \\in S} P(s' \\mid s,a) \\max_{\\pi} v_{\\pi}(s') \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2.3.2. Optimal Policy\n",
    "\n",
    "- The optimal policy is the policy that achieves the highest value for every state\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\pi_{*}(s) = \\arg\\max_{\\pi} v_{\\pi}(s)$$\n",
    "\n",
    "\n",
    "$\\quad \\;\\;$and its optimal value function is written $v_{*}(s)$\n",
    "\n",
    "<br>\n",
    "\n",
    "- An optimal policy can be found by maximizing over $q_{*} (s,a)$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\pi_{*} (a \\mid s) = \n",
    "\\begin{cases}\n",
    "1 \\quad \\text{if } a = \\arg \\max_{a \\in A} q_{*}(s,a)\\\\\n",
    "0 \\quad \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "- There is always a deterministic optimal policy for any MDP\n",
    "\n",
    "\n",
    "- If we know $\\pi_{*} (a \\mid s)$, we immediately have the opimal policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Solving the Bellman Optimality Equation\n",
    "\n",
    "\n",
    "We can directly define the optimal value function using Bellman optimality equation\n",
    "\n",
    "<br>\n",
    "\n",
    "$$v_* (s) = R(s) + \\gamma \\max_{a} \\sum_{s' \\in S} P(s' \\mid s, a) \\;v_* \\left(s' \\right)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "and optimal policy is simply the action that attains this max\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\pi_*(s) = \\arg\\max_{a} \\sum_{s' \\in S}  P\\left(s' \\mid s,a \\right) \\, v_* \\left(s'\\right)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "- Bellman Optimality Equation is non-linear\n",
    "\n",
    "\n",
    "- No closed form solution (in general)\n",
    "\n",
    "\n",
    "- (Will learn later) many iterative solution methods\n",
    "    - Value Iteration\n",
    "    - Policy Iteration\n",
    "    - Q-learning\n",
    "    - SARSA\n",
    "\n",
    "\n",
    "- You will get into details in the course of reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1. Value Iteration \n",
    "\n",
    "\n",
    "Algorithm\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\quad$ 1. Initialize an estimate for the value function arbitrarily (or zeros)\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ v(s) \\; \\leftarrow \\; 0 \\quad \\forall s \\in S $$\n",
    "\n",
    "$\\quad$ 2. Repeat, update\n",
    "\n",
    "<br>\n",
    "\n",
    "$$v (s) \\; \\leftarrow \\; R(s) + \\gamma \\max_{a} \\sum_{s' \\in S} P(s' \\mid s,a) \\;v \\left(s' \\right), \\quad \\forall s \\in S$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Note\n",
    "\n",
    "- If we know the solution to subproblems $v_* \\left(s' \\right)$\n",
    "\n",
    "\n",
    "- Then solution $v_* \\left(s' \\right)$ can be found by one-step lookahead\n",
    "\n",
    "<br>\n",
    "\n",
    "$$v (s) \\; \\leftarrow \\; R(s) + \\gamma \\max_{a} \\sum_{s' \\in S} P\\left(s' \\mid s,a\\right) \\;v \\left(s' \\right), \\quad \\forall s \\in S$$\n",
    "\n",
    "<br>\n",
    "\n",
    "- The idea of value iteration is to apply these updates iteratively\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2. Policy Iteration\n",
    "\n",
    "Algorithm\n",
    "\n",
    "$\\quad$ 1. initialize policy $\\hat{\\pi}$ (e.g., randomly)\n",
    "\n",
    "\n",
    "$\\quad$ 2. Compute a value function of policy, $v_{\\pi}$ (e.g., via solving linear system or Bellman expectation equation iteratively)\n",
    "\n",
    "$\\quad$ 3. Update $\\pi$ to be _greedy_ policy with respect to $v_{\\pi}$\n",
    "<br><br>\n",
    "\n",
    "$$\\pi(s) \\leftarrow \\arg\\max_{a} \\sum_{s' \\in S}P \\left(s' \\mid s,a \\right) v_{\\pi}\\left(s'\\right)$$\n",
    "\n",
    "$\\quad$ 4. If policy $\\pi$ changed in last iteration, return to step 2\n",
    "\n",
    "<br><br>\n",
    "<center><img src=\"./image_files/policy_iter.png\" width = 350></center>\n",
    "<br>\n",
    "\n",
    "Note\n",
    "\n",
    "- Given a policy $\\pi$, then evaluate the policy $\\pi$\n",
    "    \n",
    "- Improve the policy by acting greedily with respect to $v_{\\pi}$\n",
    "\n",
    "\n",
    "- Policy iteration requires fewer iterations that value iteration, but each iteration requires solving a linear system instead of just applying Bellman operator\n",
    "\n",
    "- In practice, policy iteration is often faster, especially if the transition probabilities are structured (e.g., sparse) to make solution of linear system efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe src=\"https://www.youtube.com/embed/XLEpZzdLxI4?rel=0\" \n",
       " width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe></center>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<center><iframe src=\"https://www.youtube.com/embed/XLEpZzdLxI4?rel=0\" \n",
    " width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe src=\"https://www.youtube.com/embed/4R6kDYDcq8U?rel=0\" \n",
       "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe></center>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<center><iframe src=\"https://www.youtube.com/embed/4R6kDYDcq8U?rel=0\" \n",
    "width=\"560\" height=\"315\" frameborder=\"0\" allowfullscreen></iframe></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Example\n",
    "\n",
    "<br>\n",
    "<img src = \"./image_files/MDP_SA_example.png\" width = 500> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define MDP as a two-level dictionary__\n",
    "\n",
    "`P` is a two-level dictionary where the first key is the state and the second key is the action.\n",
    "\n",
    "- State indices `[0, 1, 2, 3]` correspond to [PU, PF, RU, RF]\n",
    "- Action indices `[0, 1]` correspond to [Saving momey, Advertising]\n",
    "\n",
    "\n",
    "`P[state][action]` is a list of tuples `(probability, nextstate)`.\n",
    "\n",
    "For example, \n",
    "\n",
    "- the transition information for s = 0, a = 0 is `P[0][0] = [(1, 0)]`\n",
    "- the transition information for s = 3, a = 0 is `P[3][0] = [(0.5, 2), (0.5, 3)]`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T05:08:53.881200Z",
     "start_time": "2021-04-29T05:08:53.872145Z"
    }
   },
   "outputs": [],
   "source": [
    "P = {    \n",
    "    0: {0: [(1, 0)], # 현재 state가 0이고 action 0번(saving money) 했을 때 1의 확률로 0으로 오게된다.\n",
    "        1: [(0.5, 0), (0.5, 1)]},\n",
    "    1: {0: [(0.5, 0), (0.5, 3)], \n",
    "        1: [(1, 1)]},\n",
    "    2: {0: [(0.5, 0), (0.5, 2)], \n",
    "        1: [(0.5, 0), (0.5, 1)]},\n",
    "    3: {0: [(0.5, 2), (0.5, 3)], \n",
    "        1: [(1, 1)]},\n",
    "}\n",
    "\n",
    "R = [0, 0, 10, 10]\n",
    "gamma = 0.9\n",
    "\n",
    "States = [0, 1, 2, 3]\n",
    "Actions = [0, 1]\n",
    "\n",
    "v = [0]*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sum_{s' \\in S} p\\left(s\\ \\mid s,a\\right) \\;v \\left(s' \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T05:08:54.418236Z",
     "start_time": "2021-04-29T05:08:54.411619Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5, 0), (0.5, 2)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T05:08:54.621493Z",
     "start_time": "2021-04-29T05:08:54.615040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "# compute the above summation\n",
    "\n",
    "v = [0, 0, 10, 10]\n",
    "\n",
    "temp = 0\n",
    "for trans in P[2][0]:\n",
    "    temp = temp + trans[0]*v[trans[1]] # 0.5 * v[0] + 0.5 * v[2] = 0.5 * 0 + 0.5 * 10 = 5\n",
    "    \n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T05:08:54.862456Z",
     "start_time": "2021-04-29T05:08:54.855114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shorten\n",
    "\n",
    "sum(trans[0]*v[trans[1]] for trans in P[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Value Iteration\n",
    "\n",
    "<br>\n",
    "\n",
    "$$v_* (s) = R(s) + \\gamma \\max_{a} \\sum_{s' \\in S} P(s' \\mid s, a) \\;v_* \\left(s' \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T05:09:11.840318Z",
     "start_time": "2021-04-29T05:09:11.830085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.58508953413495, 38.60400287377479, 44.02416232966445, 54.20158563176306]\n"
     ]
    }
   ],
   "source": [
    "# optimal value fuction\n",
    "\n",
    "v = [0]*4\n",
    "\n",
    "for i in range(100):\n",
    "    for s in States: # 모든 state에 대해 for문\n",
    "        q_0 = sum(trans[0]*v[trans[1]] for trans in P[s][0]) # 0으로 action을 취했을 때 sigma한 값\n",
    "        q_1 = sum(trans[0]*v[trans[1]] for trans in P[s][1]) # 1으로 action을 취했을 때 sigma한 값\n",
    "\n",
    "        v[s] = R[s] + gamma*max(q_0, q_1) # a가 0일 때와 1일때 큰 값을 선택 -> optimal\n",
    "        # 특정 policy를 따르는 value를 구해서 그 value들 중에 더 높은값을 고르는 쪽으로 선택을 하도록 만들어줌\n",
    "    \n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "$$\\pi^*(s) = \\arg\\max_{a} \\sum_{s' \\in S}  p\\left(s' \\mid s,a\\right) \\, v_{\\pi}\\left(s'\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T05:09:19.068977Z",
     "start_time": "2021-04-29T05:09:19.060684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# optimal policy\n",
    "\n",
    "# once v computed\n",
    "\n",
    "optPolicy = [0]*4\n",
    "\n",
    "for s in States:\n",
    "    q_0 = sum(trans[0]*v[trans[1]] for trans in P[s][0])\n",
    "    q_1 = sum(trans[0]*v[trans[1]] for trans in P[s][1])   \n",
    "    \n",
    "    optPolicy[s] = np.argmax([q_0, q_1]) # 2개 중에 높은 index를 선택\n",
    "\n",
    "print(optPolicy)    # 첫번째 state에서는 1번(advertisement)을 선택, 나머지 state에서는 0번(saving money)이 최적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T05:19:27.185937Z",
     "start_time": "2021-04-29T05:19:27.176617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.58508953413495, 38.60400287377479, 44.02416232966445, 54.20158563176306]\n"
     ]
    }
   ],
   "source": [
    "# shorten\n",
    "\n",
    "v = [0]*4\n",
    "\n",
    "for i in range(100):\n",
    "    for s in States:\n",
    "        v[s] = R[s] + gamma*max([sum([trans[0]*v[trans[1]] for trans in P[s][a]]) for a in Actions])    \n",
    "    \n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T05:19:27.636934Z",
     "start_time": "2021-04-29T05:19:27.629825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "optPolicy = [0]*4\n",
    "\n",
    "for s in States:       \n",
    "    optPolicy[s] = np.argmax([sum([trans[0]*v[trans[1]] for trans in P[s][a]]) for a in Actions])\n",
    "\n",
    "print(optPolicy)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Policy Iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T05:19:50.227913Z",
     "start_time": "2021-04-29T05:19:50.220155Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = np.random.randint(0,2,4) # policy를 랜덤하게 initialize\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T05:19:58.103701Z",
     "start_time": "2021-04-29T05:19:58.097105Z"
    }
   },
   "outputs": [],
   "source": [
    "def cal_value(policy):\n",
    "    v = [0]*4\n",
    "    for i in range(100):\n",
    "        for s in States:\n",
    "            q = sum(trans[0]*v[trans[1]] for trans in P[s][policy[s]])\n",
    "            v[s] = R[s] + gamma*q\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T05:20:05.224895Z",
     "start_time": "2021-04-29T05:20:05.218520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29.362497240834138, 35.887497566214655, 39.36249766317196, 50.387497756266676]\n"
     ]
    }
   ],
   "source": [
    "V_pi = cal_value(policy)\n",
    "print(V_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T05:20:09.370132Z",
     "start_time": "2021-04-29T05:20:09.337152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10): \n",
    "    for s in range(4): \n",
    "        values_temp = []\n",
    "        for action in [0,1]: \n",
    "            temp = 0\n",
    "            for trans in P[s][action]: \n",
    "                temp = temp + trans[0]*V_pi[trans[1]]\n",
    "            values_temp.append(temp) \n",
    "\n",
    "        policy[s] = np.argmax(values_temp) \n",
    "        V_pi = cal_value(policy)\n",
    "\n",
    "optPolicy = policy\n",
    "\n",
    "print(optPolicy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exercise (Gridworld Domain)\n",
    "\n",
    "- Simple grid world with a goal state with reward and a “bad state” with reward -100\n",
    "- Actions move in the desired direction with probably 0.8, in one of the perpendicular directions with\n",
    "- Taking an action that would bump into a wall leaves agent where it is\n",
    "\n",
    "<img src = \"./image_files/gridworld.png\" width = 500> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T05:21:13.363723Z",
     "start_time": "2021-04-29T05:21:13.339784Z"
    }
   },
   "outputs": [],
   "source": [
    "P = {\n",
    "     0: {0: [(0.9,0), (0.1,1), (0,4)], \n",
    "         1: [(0.8,1), (0.1,4), (0.1,0)], \n",
    "         2: [(0.8,4), (0.1,1), (0.1,0)], \n",
    "         3: [(0.9,0), (0.1,4)]},\n",
    "     1: {0: [(0.8,1), (0.1,2), (0.1,0)], \n",
    "         1: [(0.8,2), (0.2,1)], \n",
    "         2: [(0.8,1), (0.1,0), (0.1,2)], \n",
    "         3: [(0.8,0), (0.2,1)]},\n",
    "     2: {0: [(0.8,2), (0.1,3), (0.1,1)], \n",
    "         1: [(0.8,3), (0.1,5), (0.1,2)], \n",
    "         2: [(0.8,5), (0.1,1), (0.1,3)], \n",
    "         3: [(0.8,1), (0.1,2), (0.1,5)]},\n",
    "     3: {0: [(0.9,3), (0.1,2)], \n",
    "         1: [(0.9,3), (0.1,6)], \n",
    "         2: [(0.8,6), (0.1,2), (0.1,3)], \n",
    "         3: [(0.8,2), (0.1,3), (0.1,6)]},\n",
    "     4: {0: [(0.8,0), (0.2,4)], \n",
    "         1: [(0.8,4), (0.1,7), (0.1,0)], \n",
    "         2: [(0.8,7), (0.2,4)], \n",
    "         3: [(0.8,4), (0.1,0), (0.1,7)]},\n",
    "     5: {0: [(0.8,2), (0.1,6), (0.1,5)], \n",
    "         1: [(0.8,6), (0.1,9), (0.1,2)], \n",
    "         2: [(0.8,9), (0.1,5), (0.1,6)], \n",
    "         3: [(0.8,5), (0.1,2), (0.1,9)]},\n",
    "     6: {0: [(0.8,3), (0.1,6), (0.1,5)], \n",
    "         1: [(0.8,6), (0.1,10), (0.1,3)], \n",
    "         2: [(0.8,10), (0.1,5), (0.1,6)], \n",
    "         3: [(0.8,5), (0.1,3), (0.1,10)]},\n",
    "     7: {0: [(0.8,4), (0.1,8), (0.1,7)], \n",
    "         1: [(0.8,8), (0.1,7), (0.1,4)], \n",
    "         2: [(0.9,7), (0.1,8)], \n",
    "         3: [(0.9,7), (0.1,4)]},\n",
    "     8: {0: [(0.8,8), (0.1,9), (0.1,7)], \n",
    "         1: [(0.8,9), (0.2,8)], \n",
    "         2: [(0.8,8), (0.1,7), (0.1,9)], \n",
    "         3: [(0.8,7), (0.2,8)]},\n",
    "     9: {0: [(0.8,5), (0.1,10), (0.1,8)], \n",
    "         1: [(0.8,9), (0.1,9), (0.1,5)], \n",
    "         2: [(0.8,9), (0.1,8), (0.1,10)], \n",
    "         3: [(0.8,8), (0.1,5), (0.1,9)]},\n",
    "     10: {0: [(0.8,6), (0.1,10), (0.1,9)], \n",
    "          1: [(0.9,10), (0.1,6)], \n",
    "          2: [(0.9,10), (0.1,9)], \n",
    "          3: [(0.8,9), (0.1,6), (0.1,10)]}\n",
    "}\n",
    "\n",
    "R = [0, 0, 0, 1, 0, 0, -100, 0, 0, 0, 0]\n",
    "gamma = 0.9\n",
    "\n",
    "States = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "Actions = [0, 1, 2, 3] # [north, east, south, west]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T05:21:13.750760Z",
     "start_time": "2021-04-29T05:21:13.734551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.46991289990088, 6.313016781079707, 7.189835364530538, 8.668832766371658, 4.8028486314273, 3.346646443535637, -96.67286272722137, 4.161433444369266, 3.6539401768050603, 3.2220160316109103, 1.526193402980731]\n"
     ]
    }
   ],
   "source": [
    "# optimal value fuction\n",
    "\n",
    "v = [0]*11\n",
    "\n",
    "for i in range(100):\n",
    "    for s in States:\n",
    "        v[s] = R[s] + gamma*max([sum([trans[0]*v[trans[1]] for trans in P[s][a]]) for a in Actions])  \n",
    "    \n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T05:21:14.168343Z",
     "start_time": "2021-04-29T05:21:14.160977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 0, 3, 3, 0, 3, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "# optimal policy\n",
    "\n",
    "optPolicy = [0]*11\n",
    "\n",
    "for s in States:       \n",
    "    optPolicy[s] = np.argmax([sum([trans[0]*v[trans[1]] for trans in P[s][a]]) for a in Actions])\n",
    "\n",
    "print(optPolicy)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./image_files/gridworld_answer.png\" width = 300> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
