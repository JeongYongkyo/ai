{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<font size='6'><b>Markov Reward Process</b></font><br><br>\n",
    "\n",
    "<br>\n",
    "<div class=pull-right>\n",
    "By Prof. Seungchul Lee<br>\n",
    "http://iai.postech.ac.kr/<br>\n",
    "Industrial AI Lab at POSTECH\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source\n",
    "\n",
    "- By David Silver's RL Course at UCL\n",
    "- By Prof. Zico Kolter at CMU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Table of Contents\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Markov Reward Process\n",
    "\n",
    "\n",
    "- Suppose that each transition in a Markov chain is associated with a reward, $r$\n",
    "\n",
    "\n",
    "- As the Markov chain proceeds from state to state, there is an associated sequence of rewards\n",
    "\n",
    "\n",
    "- Discount factor $\\gamma$\n",
    "\n",
    "\n",
    "- Later, we will study dynamic programming and Markov decision theory $\\implies$ Markov Decision Process (MDP)\n",
    "    \n",
    "\n",
    "\n",
    "## 1.1. Definition of MRP\n",
    "\n",
    "Definition: A Markov Reward Process is a tuple $\\langle S,P,R,\\gamma \\rangle$\n",
    "\n",
    "- $S$ is a finite set of states\n",
    "- $P$ is a state transition probability matrix\n",
    "\n",
    "$$P_{ss'} = P\\left[S_{t+1} = s' \\mid S_t = s \\right]$$\n",
    "\n",
    "- $R$ is a reward function, $R = \\mathbb{E} \\left[ R_{t+1} \\mid S_t = s \\right]$\n",
    "- $\\gamma$ is a discount factor, $\\gamma \\in [0,1]$\n",
    "\n",
    "<br>\n",
    "\n",
    "Example: student MRP \n",
    "\n",
    "<img src = \"./image_files/student_MRP.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Reward over Multiple Transitions (= Return $G_t$)\n",
    "\n",
    "Definition: The return $G_t$ is the total discounted reward from time-step $t$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots = \\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Discount factor $\\gamma$\n",
    "\n",
    "- It is reasonable to maximize the sum of rewards\n",
    "\n",
    "\n",
    "- It is also reasonable to prefer rewards now to rewards later\n",
    "\n",
    "\n",
    "- One solution: values of rewards decay exponentially\n",
    "\n",
    "\n",
    "- Mathematically convenient (avoid infinite returns and values)\n",
    "\n",
    "\n",
    "- Human often acts as if there is a discount factor $\\gamma < 1$\n",
    "    - $\\gamma = 0$: Only care about immediate reward\n",
    "    - $\\gamma = 1$: Future reward is as beneficial as immediate reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T02:37:43.452572Z",
     "start_time": "2021-04-29T02:37:43.318041Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T02:42:31.999994Z",
     "start_time": "2021-04-29T02:42:31.992679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.0032\n"
     ]
    }
   ],
   "source": [
    "# [C1 C2 C3 Pass Pub FB Sleep] = [0 1 2 3 4 5 6]\n",
    "\n",
    "R = [-2, -2, -2, 10, 1, -1, 0]\n",
    "gamma = 0.9\n",
    "\n",
    "# if a sequence is given\n",
    "S = [0, 1, 2, 4, 2, 4] # sequence가 주어졌을 경우\n",
    "\n",
    "G = 0\n",
    "for i in range(5):\n",
    "    G = G + (gamma**i)*R[S[i]] # 각 스텝일 때의 reward를 계산해서 G에 더함\n",
    "    \n",
    "print(G)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T02:42:43.932011Z",
     "start_time": "2021-04-29T02:42:43.919687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "R = [-2, -2, -2, 10, 1, -1, 0]\n",
    "gamma = 0.9\n",
    "\n",
    "P = [[0, 0.5, 0, 0, 0, 0.5, 0],\n",
    "    [0, 0, 0.8, 0, 0, 0, 0.2],\n",
    "    [0, 0, 0, 0.6, 0.4, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1],\n",
    "    [0.2, 0.4, 0.4, 0, 0, 0, 0],\n",
    "    [0.1, 0, 0, 0, 0, 0.9, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1]]\n",
    "\n",
    "# sequence generated by Markov chain\n",
    "# [C1 C2 C3 Pass Pub FB Sleep] = [0 1 2 3 4 5 6]\n",
    "\n",
    "# starting from 0\n",
    "x = 0 \n",
    "S = []\n",
    "S.append(x)\n",
    "\n",
    "for i in range(5):\n",
    "    x = np.random.choice(len(P),1,p = P[x][:])[0] #매 스텝 random choice로 이동\n",
    "    S.append(x)\n",
    "\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T02:42:44.189728Z",
     "start_time": "2021-04-29T02:42:44.183403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 6, 6]\n",
      "1.870000000000001\n"
     ]
    }
   ],
   "source": [
    "G = 0\n",
    "for i in range(5):\n",
    "    G = G + (gamma**i)*R[S[i]]\n",
    "\n",
    "print(S)      \n",
    "print(G)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Value Function\n",
    "\n",
    "<br>\n",
    "Definition: The state value function $v(s)$ of an MRP is the expected return starting from state $s$\n",
    "<br><br>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "v(s) & = \\mathbb{E}[G_t \\mid S_t = s] \\\\\n",
    "& = \\mathbb{E}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots \\mid S_t = s]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "- The value function $v(s)$ gives the long-term value of state $s$\n",
    "\n",
    "<br>\n",
    "\n",
    "- Sample returns for student MRP: starting from $S_1 = C_1$ with $\\gamma = \\frac{1}{2}$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$G_1 = R_2 + \\gamma R_3 + \\cdots + \\gamma^{T-2} R_T$$\n",
    "\n",
    "<br>\n",
    "\n",
    "- (Naive) Computing the value function in MRP\n",
    "    - Generate a large number of episodes and compute the average return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bellman Equations for MRP\n",
    "\n",
    "- The value function $v(S_t)$ can be decomposed into two parts:\n",
    "    - Immediate reward $R_{t+1}$ at state $S_t$\n",
    "    - Discounted value of successor state $\\gamma v (S_{t+1})$\n",
    "\n",
    "<br>\n",
    "$$\n",
    "\\begin{align*}\n",
    "v(s) &= \\mathbb{E} \\left[G_t \\mid S_t = s \\right] \\\\\n",
    "&= \\mathbb{E} \\left[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots \\mid S_t = s \\right] \\\\\n",
    "&= \\mathbb{E} \\left[R_{t+1} + \\gamma \\left( R_{t+2} + \\gamma R_{t+3} + \\cdots \\right) \\mid S_t = s \\right] \\\\\n",
    "&= \\mathbb{E} \\left[R_{t+1} + \\gamma G_{t+1} \\mid S_t = s \\right] \\\\\n",
    "&= \\mathbb{E} \\left[R_{t+1} + \\gamma v \\left(S_{t+1} \\right) \\mid S_t = s \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "<br><br>\n",
    "<img src = \"./image_files/BellmanMRP_diagram.png\" width = 320> \n",
    "\n",
    "- Bellman Equation for Student MRP\n",
    "<img src = \"./image_files/student_BellmanMRP.png\" width = 500> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Bellman Equation in Matrix Form\n",
    "\n",
    "<br><br>\n",
    "$$v(s) = R + \\gamma \\sum_{s' \\in S} P_{ss'}v\\left(s'\\right) \\qquad \\forall s$$\n",
    "<br>\n",
    "\n",
    "The Bellman equation can be expressed concisely using matrices,\n",
    "\n",
    "<br>\n",
    "\n",
    "$$v = R + \\gamma P v$$ where $v$ is a column vector with one entry per state\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "v(1)\\\\ \\vdots \\\\v(n)\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "R_1\\\\ \\vdots \\\\R_n\n",
    "\\end{bmatrix} + \\gamma\n",
    "\\begin{bmatrix}\n",
    "p_{11} & \\cdots & p_{1n}\\\\ &\\vdots& \\\\p_{n1}& \\cdots &p_{nn}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v(1)\\\\ \\vdots \\\\v(n)\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Solving the Bellman Equation\n",
    "\n",
    "- The Bellman equation is a linear equation\n",
    "\n",
    "\n",
    "- It can be solved directly:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "v &= R + \\gamma P v \\\\\n",
    "(I-\\gamma R) v & = R \\\\\n",
    "v & = (I - \\gamma P)^{-1}R\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "- Direct solution only possible for small MRP\n",
    "    - Computational complexity is $O(n^3)$ for $n$ states\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src = \"./image_files/student_MRP.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are many iterative methods for large MRP\n",
    "    - Dynamic programming\n",
    "    - Monte-Carlo simulation\n",
    "    - Temporal-Difference learning\n",
    "\n",
    "\n",
    "- __Iterative algorithm__ for value function (<font color='red'>Value Iteration</font>)\n",
    "\n",
    "    - Initialize $v_1(s) = 0$ for all $s$\n",
    "\n",
    "    - For $k=1$ until convergence\n",
    "\n",
    "        - for all $s$ in $S$\n",
    "\n",
    "                               \n",
    "$$v_{k+1}(s) \\;\\; \\longleftarrow \\;\\; R(s) + \\gamma \\sum_{s' \\in S} p\\left(s' \\mid s\\right) v_k \\left(s'\\right) $$        \n",
    "\n",
    "<br><br>\n",
    "<img src = \"./image_files/studentMRP_02.png\" width = 700> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T02:46:46.597996Z",
     "start_time": "2021-04-29T02:46:46.582390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.01272891]\n",
      " [ 0.9426553 ]\n",
      " [ 4.08702125]\n",
      " [10.        ]\n",
      " [ 1.90839235]\n",
      " [-7.63760843]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# [C1 C2 C3 Pass Pub FB Sleep] = [0 1 2 3 4 5 6]\n",
    "\n",
    "P = [[0, 0.5, 0, 0, 0, 0.5, 0],\n",
    "    [0, 0, 0.8, 0, 0, 0, 0.2],\n",
    "    [0, 0, 0, 0.6, 0.4, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1],\n",
    "    [0.2, 0.4, 0.4, 0, 0, 0, 0],\n",
    "    [0.1, 0, 0, 0, 0, 0.9, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1]]\n",
    "\n",
    "R = [-2, -2, -2, 10, 1, -1, 0]\n",
    "\n",
    "P = np.asmatrix(P)\n",
    "R = np.asmatrix(R)\n",
    "R = R.T\n",
    "\n",
    "gamma = 0.9\n",
    "v = np.zeros([7, 1])\n",
    "\n",
    "# compute value using Inverse matrix, gamma = 0.9\n",
    "\n",
    "v = (np.eye(7) - gamma*P).I*R\n",
    "\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T02:47:03.937393Z",
     "start_time": "2021-04-29T02:47:03.901928Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.01272891]\n",
      " [ 0.9426553 ]\n",
      " [ 4.08702125]\n",
      " [10.        ]\n",
      " [ 1.90839235]\n",
      " [-7.63760843]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# compute value using iteration, gamma = 0.9\n",
    "\n",
    "for i in range(1000):\n",
    "    v = R + gamma*P*v\n",
    "    \n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./image_files/studentMRP_03.png\" width = 700> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T04:00:51.151236Z",
     "start_time": "2021-04-29T04:00:51.110993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-12.5]\n",
      " [  1.5]\n",
      " [  4.3]\n",
      " [ 10. ]\n",
      " [  0.8]\n",
      " [-22.5]\n",
      " [  0. ]]\n"
     ]
    }
   ],
   "source": [
    "# compute value using iteration, gamma = 1\n",
    "\n",
    "gamma = 1\n",
    "v = np.zeros([7, 1])\n",
    "\n",
    "for i in range(1000):\n",
    "    v = R + gamma*P*v\n",
    "    \n",
    "print(np.round(v,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
